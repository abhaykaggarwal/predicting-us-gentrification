{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt \n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_geography_model(data_df, predict_df, df_cols, predict_cols, name):\n",
    "    \"\"\"\n",
    "    Note that predict_cols[0] must be equal to the binary gentrification varible\n",
    "    \"\"\"\n",
    "    # drop na\n",
    "    d = {}\n",
    "    for i in predict_cols:\n",
    "        d[i] = predict_df[i]\n",
    "    # merge into new df\n",
    "    df = data_df.join(pd.DataFrame(\n",
    "        #index=\"geo_fips\",\n",
    "        data=d\n",
    "    ))\n",
    "    s = df.shape\n",
    "    df = df.dropna()\n",
    "    print(\"previous to dropping\", s, \"after dropping\", df.shape)\n",
    "    new_predict = pd.DataFrame()\n",
    "    for i in predict_cols:\n",
    "        new_predict[i] = df[i]\n",
    "    del data_df\n",
    "    # new data_df\n",
    "    data_df = pd.DataFrame()\n",
    "    for i in df_cols + [\"geo_fips\"]:\n",
    "        data_df[i] = df[i]\n",
    "    print(new_predict.shape, data_df.shape)\n",
    "    assert new_predict.shape[0] == data_df.shape[0], \"after removing nulls, predict and data don't have the same # of rows\"\n",
    "    # normal\n",
    "    for i in predict_cols:\n",
    "        print(\"=============================================================\")\n",
    "        if \"disp\" in i.lower():\n",
    "            print(i, \"regression\")\n",
    "            model, stats = train_model(data_df[df_cols], new_predict, df_cols, i, \"regression\")\n",
    "            diff = stats.loc[(stats['diff'] >= stats[\"diff\"].quantile(0.01)) & (stats['diff'] <= stats[\"diff\"].quantile(0.99))][\"diff\"]\n",
    "            print(\"description\")\n",
    "            print(stats['diff'].describe())\n",
    "            #plt.hist(stats[\"diff\"])\n",
    "            plt.hist(diff)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(i, \"classification\")\n",
    "            model, stats, confusion_matrix = train_model(data_df[df_cols], new_predict, df_cols, i, \"classification\")\n",
    "            print(\"accuracy score\", stats)\n",
    "            print(\"confusion matrix\")\n",
    "            print(confusion_matrix)\n",
    "            output_csv(model, data_df, df_cols, new_predict[i], \"Normal set\", name)\n",
    "    # equal number of gentrified and non-gent geometries \n",
    "    original_data = data_df.copy()\n",
    "    original_predict = new_predict.copy()\n",
    "    temp_table = data_df.copy()\n",
    "    temp_table[predict_cols[0]] = new_predict[predict_cols[0]]\n",
    "    temp_table = temp_table.loc[temp_table[predict_cols[0]] == 1]\n",
    "    gent_indicies = temp_table.index\n",
    "    other_indicies = set(data_df.index) - set(gent_indicies)\n",
    "    non_gent_selected_indicies = random.sample(list(other_indicies), len(gent_indicies))\n",
    "    assert len(non_gent_selected_indicies) == len(gent_indicies), \"gent data is not split 50/50\"\n",
    "    print(list(non_gent_selected_indicies)[:5], type(non_gent_selected_indicies), type(gent_indicies))\n",
    "    gent_indicies = list(gent_indicies)\n",
    "    data_df = data_df.loc[non_gent_selected_indicies + gent_indicies]\n",
    "    new_predict = new_predict.loc[non_gent_selected_indicies + gent_indicies]\n",
    "    #validation = \n",
    "    print(\"=============================================================\")\n",
    "    print(\"=============================================================\")\n",
    "    print(\"=============================================================\")\n",
    "    print(\"=====================50/50 split=============================\")\n",
    "    for i in predict_cols:\n",
    "        print(\"=============================================================\")\n",
    "        if \"disp\" in i.lower():\n",
    "            print(i, \"regression\", \"50/50 split\")\n",
    "            model, stats = train_model(data_df[df_cols], new_predict, df_cols, i, \"regression\")\n",
    "            diff = stats.loc[(stats['diff'] >= stats[\"diff\"].quantile(0.01)) & (stats['diff'] <= stats[\"diff\"].quantile(0.99))][\"diff\"]\n",
    "            print(\"description\")\n",
    "            print(stats['diff'].describe())\n",
    "            #plt.hist(stats[\"diff\"])\n",
    "            plt.hist(diff)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(i, \"classification\" , \"50/50 split\")\n",
    "            model, stats, confusion_matrix = train_model(data_df[df_cols], new_predict, df_cols, i, \"classification\")\n",
    "            print(\"accuracy score\", stats)\n",
    "            print(\"confusion matrix\")\n",
    "            print(confusion_matrix)\n",
    "            output_csv(model, original_data, df_cols, original_predict[i], \"Even split\" ,name)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_classification(x1_train, y1_train):\n",
    "    def helper(depth):\n",
    "        #clf = tree.DecisionTreeClassifier(max_depth=50, criterion=\"entropy\")\n",
    "        clf = tree.DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n",
    "        clf = clf.fit(x1_train, y1_train)\n",
    "        z = clf.predict(x1_train)\n",
    "        stats = accuracy_score(z, y1_train)\n",
    "        matrix = confusion_matrix(y1_train, z) \n",
    "        return clf, stats, matrix\n",
    "    max_depth = None\n",
    "    clf = None\n",
    "    max_accuracy = 0\n",
    "    max_determinant = 0\n",
    "    for i in range(10,50,5):\n",
    "        model, acc, matrix = helper(i)\n",
    "        #if acc > max_accuracy:\n",
    "        det = np.linalg.det(matrix)\n",
    "        if det > max_determinant:\n",
    "            clf = model\n",
    "            max_accuracy = acc\n",
    "            max_depth = i\n",
    "            max_determinant = det\n",
    "    return clf, max_depth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_regression(x1_train, y1_train):\n",
    "    def helper(depth):\n",
    "        clf = tree.DecisionTreeRegressor(max_depth=depth, criterion=\"entropy\")\n",
    "        clf = clf.fit(x1_train, y1_train)\n",
    "        z = clf.predict(x1_train)\n",
    "        stats = accuracy_score(z, y1_train)\n",
    "        matrix = confusion_matrix(y1_train, z) \n",
    "        return clf, stats, matrix\n",
    "    max_depth = None\n",
    "    clf = None\n",
    "    max_accuracy = 0\n",
    "    max_determinant = 0\n",
    "    for i in range(10,50,5):\n",
    "        model, acc, matrix = helper(i)\n",
    "        #if acc > max_accuracy:\n",
    "        det = np.linalg.det(matrix)\n",
    "        if det > max_determinant:\n",
    "            clf = model\n",
    "            max_accuracy = acc\n",
    "            max_depth = i\n",
    "            max_determinant = det\n",
    "    return clf, max_depth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, predict_df, df_cols, predict_col, model_type):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    df_norm = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df_cols)\n",
    "    X = df_norm\n",
    "    Y = predict_df[predict_col]\n",
    "    #x1_train, x1_val, y1_train, y1_val = train_test_split(X, Y, test_size=0.30)\n",
    "    x1_train, x1_val, y1_train, y1_val = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    print(\"size of training set\", str(y1_train.shape), \"size of testing set\", len(y1_val), \"original\", str(Y.shape))\n",
    "    # clean Data\n",
    "    x1_train, y1_train = clean_data(x1_train, y1_train)\n",
    "    \n",
    "    if model_type == \"regression\":\n",
    "        clf = tree.DecisionTreeRegressor() # max_depth=50\n",
    "        #clf = clf.fit(x1_train._get_numeric_data(), y1_train)\n",
    "        clf = clf.fit(x1_train, y1_train)\n",
    "        #clf, max_depth = optimize_regression(x1_train, y1_train)\n",
    "        z = clf.predict(x1_val)\n",
    "        stats = pd.DataFrame({\"predicted\": z, \"actual\": y1_val, \"diff\": (y1_val - z)})\n",
    "        return clf, stats\n",
    "    else:\n",
    "        #clf = tree.DecisionTreeClassifier(max_depth=50, criterion=\"entropy\")\n",
    "        #clf = clf.fit(x1_train, y1_train)\n",
    "        clf, max_depth = optimize_classification(x1_train, y1_train)\n",
    "        z = clf.predict(x1_val)\n",
    "        #tree.plot_tree(clf)\n",
    "        from sklearn.externals.six import StringIO  \n",
    "        from IPython.display import Image  \n",
    "        from sklearn.tree import export_graphviz\n",
    "        import pydotplus\n",
    "        dot_data = StringIO()\n",
    "        export_graphviz(clf, out_file=dot_data,  \n",
    "                        filled=True, rounded=True,\n",
    "                        special_characters=True)\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "        display(Image(graph.create_png()))\n",
    "        \n",
    "        stats = accuracy_score(z, y1_val)\n",
    "        confusion = confusion_matrix(y1_val, z) \n",
    "        return clf, stats, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(a, b):\n",
    "    \"\"\"\n",
    "    a must be from data, b must be from model\n",
    "    \"\"\"\n",
    "    assert a in [0, 1] and b in [0, 1], \"not binary 0/1\"\n",
    "    if a and b:\n",
    "        #print(\"TP\")\n",
    "        return 0\n",
    "    elif a and not b:\n",
    "        #print(\"FN\")\n",
    "        return 1\n",
    "    elif not a and b:\n",
    "        #print(\"FP\")\n",
    "        return 2\n",
    "    elif not a and not b:\n",
    "        #print(\"TN\")\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_csv(clf, data, df_cols, predict, split_type, name):\n",
    "    z = clf.predict(data[df_cols])\n",
    "    #stats = accuracy_score(z, predict)\n",
    "    #print(data.columns)\n",
    "    #fips = data[\"geo_fips\"]\n",
    "    #print(data[\"geo_fips\"])\n",
    "    df = pd.DataFrame({\"actual\": predict, \"predicted\": z, \"geo_fips\": data[\"geo_fips\"]})\n",
    "    arr = []\n",
    "    for index, row in df.iterrows():\n",
    "        #print(row.get(\"geo_fips\"))\n",
    "        #print(df.loc[index,'Qty'] == 1)\n",
    "        \"\"\"\n",
    "        val = None;\n",
    "        TP = df.loc[index,'actual'] == 1 and df.loc[index,'predicted'] == 1 #np.sum(np.logical_and(pred_labels == 1, true_labels == 1))\n",
    "        TN = df.loc[index,'actual'] == 0 and df.loc[index,'predicted'] == 0\n",
    "        #np.sum(np.logical_and(pred_labels == 0, true_labels == 0))\n",
    "        FP = df.loc[index,'actual'] == 1 and df.loc[index,'predicted'] == 0\n",
    "        #np.sum(np.logical_and(pred_labels == 1, true_labels == 0))\n",
    "        FN = df.loc[index,'actual'] == 0 and df.loc[index,'predicted'] == 1\n",
    "        #np.sum(np.logical_and(pred_labels == 0, true_labels == 1))\n",
    "        if TP:\n",
    "            val = 0\n",
    "        elif TN:\n",
    "            val = 3\n",
    "        elif FP:\n",
    "            val = 2\n",
    "        elif FN:\n",
    "            val = 1\n",
    "        arr.append(val)\n",
    "        \"\"\"\n",
    "        arr.append(confusion(df.loc[index,'actual'], df.loc[index,'predicted']))\n",
    "    df[\"false_positive\"] = arr\n",
    "    \n",
    "    df.to_csv(\"output/\" + name + \" \" + split_type + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x_train, y_train):\n",
    "    #print(type(x_train), type(y_train))\n",
    "    x_train[\"y_train\"] = y_train\n",
    "    first = x_train[\"y_train\"].quantile(0.1)\n",
    "    second = x_train[\"y_train\"].quantile(0.9)\n",
    "    df = x_train.loc[(x_train['y_train'] >= first) & (x_train['y_train'] <= second)]\n",
    "    #print(df.columns)\n",
    "    y_train = df[\"y_train\"]\n",
    "    df = df.drop(columns=[\"y_train\"])\n",
    "    return df, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data(df1, cols1, df2, cols2, predict, predict_cols1, predict_cols2):\n",
    "    assert len(cols1) == len(cols2), \"input sizes are not the same\"\n",
    "    df1 = df1[cols1]\n",
    "    df2 = df2[cols2]\n",
    "    stacked_data = np.vstack((df1.values, df2.values))\n",
    "    predict1 = predict[predict_cols1]\n",
    "    predict2 = predict[predict_cols2]\n",
    "    stacked_predict = np.vstack((predict1.values, predict2.values))\n",
    "    stacked_data = pd.DataFrame(stacked_data, columns=cols1)\n",
    "    stacked_predict = pd.DataFrame(stacked_predict, columns=predict_cols1)\n",
    "    return stacked_data, stacked_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_data_diff_geographies(df1, cols1, df2, cols2, predict1, predict2, predict_cols1, predict_cols2):\n",
    "    assert len(cols1) == len(cols2), \"input sizes are not the same\"\n",
    "    df1 = df1[cols1]\n",
    "    df2 = df2[cols2]\n",
    "    stacked_data = np.vstack((df1.values, df2.values))\n",
    "    predict1 = predict1[predict_cols1]\n",
    "    predict2 = predict2[predict_cols2]\n",
    "    stacked_predict = np.vstack((predict1.values, predict2.values))\n",
    "    stacked_data = pd.DataFrame(stacked_data, columns=cols1)\n",
    "    stacked_predict = pd.DataFrame(stacked_predict, columns=predict_cols1)\n",
    "    return stacked_data, stacked_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trtid10', 'hinc_00', 'pop_00', 'nhwhite_00', 'nhblk_00',\n",
       "       'asian_00', 'hisp_00', 'hh_00', 'hu_00', 'ohu_00', 'rhu_00',\n",
       "       'per_nonwhite_00', 'per_nhblk_00', 'per_hisp_00', 'per_asian_00',\n",
       "       'col_00', 'per_col_00', 'per_carcommute_00', 'per_rent_00',\n",
       "       '_merge_00', 'mrent_90', 'mhval_90', 'rentocc_90', 'ownocc_90',\n",
       "       '_merge_90', 'pop_90', 'nhwhite_90', 'nhblk_90', 'asian_90',\n",
       "       'hisp_90', 'hh_90', 'hinc_90', 'hu_90', 'ohu_90', 'rhu_90',\n",
       "       'per_nonwhite_90', 'per_nhblk_90', 'per_hisp_90', 'per_asian_90',\n",
       "       'col_90', 'per_col_90', 'per_carcommute_90', 'per_rent_90',\n",
       "       'units_pre50_90', 'per_units_pre50_90', '_merge_17', 'pop_17',\n",
       "       'pop_17_se', 'nhwhite_17', 'hh_17', 'hinc_17', 'hinc_17_se',\n",
       "       'hu_17', 'ohu_17', 'rhu_17', 'hu_17_se', 'rhu_17_se', 'mrent_17',\n",
       "       'mrent_17_se', 'mhval_17', 'mhval_17_se', 'per_nonwhite_17',\n",
       "       'per_nhblk_17', 'per_hisp_17', 'per_asian_17', 'col_17',\n",
       "       'per_col_17', 'per_carcommute_17', 'per_rent_17',\n",
       "       'per_units_pre50_17', 'per_built_00_17', 'per_rent_17_se',\n",
       "       'col_17_se', 'per_col_17_se', '_merge_emp', 'tot_jobs',\n",
       "       '_merge_larea', 'larea', 'downtown', '_merge_mr00', 'mrent_00',\n",
       "       'mhval_00', 'rentocc_00', 'ownocc_00', 'real_hinc_90',\n",
       "       'real_mhval_90', 'real_mrent_90', 'real_hinc_00', 'real_mhval_00',\n",
       "       'real_mrent_00', '_merge_inc', 'trtid', 'GEOID', 'NAME', 'QName',\n",
       "       'STATE', 'COUNTY', 'NATION', 'CT', 'hinc_16', '_merge_hinc',\n",
       "       'regmedian_income90', 'vli_90', 'li_90', 'mi_90', 'hmi_90',\n",
       "       'hi_90', 'vhi_90', 'vli_00', 'li_00', 'mi_00', 'hmi_00', 'hi_00',\n",
       "       'vhi_00', 'vli_17', 'li_17', 'mi_17', 'hmi_17', 'hi_17', 'vhi_17',\n",
       "       '_merge_15mover', 'reg_medinc_17', 'moveinincd_17',\n",
       "       'per_limove_17', '_merge_09mover', 'per_limove_09', 'co_medinc_09',\n",
       "       'moveinincd_09', 'popflag17', 'popflag00', 'popflag90',\n",
       "       'per_all_li_17', 'per_all_li_00', 'per_all_li_90',\n",
       "       'all_li_count_17', 'all_li_count_00', 'all_li_count_90',\n",
       "       'real_hinc_17', 'real_mhval_17', 'real_mrent_17', 'empd_17',\n",
       "       'density_17', 'OLD_ppt_ch_li_movein_09_17', 'OLD_ch_pop_00_17',\n",
       "       'OLD_pctch_pop_00_17', 'OLD_ch_per_col_00_17',\n",
       "       'OLD_pctch_per_col_00_17', 'OLD_ch_real_mhval_00_17',\n",
       "       'OLD_pctch_real_mhval_00_17', 'OLD_ch_real_mrent_00_17',\n",
       "       'OLD_pctch_real_mrent_00_17', 'OLD_ch_all_li_count_00_17',\n",
       "       'OLD_pctch_all_li_count_00_17', 'OLD_ch_per_all_li_00_17',\n",
       "       'OLD_pctch_per_all_li_00_17', 'OLD_ch_real_hinc_00_17',\n",
       "       'OLD_pctch_real_hinc_00_17', 'OLD_ch_pop_90_00',\n",
       "       'OLD_pctch_pop_90_00', 'OLD_ch_per_col_90_00',\n",
       "       'OLD_pctch_per_col_90_00', 'OLD_ch_real_mhval_90_00',\n",
       "       'OLD_pctch_real_mhval_90_00', 'OLD_ch_real_mrent_90_00',\n",
       "       'OLD_pctch_real_mrent_90_00', 'OLD_ch_all_li_count_90_00',\n",
       "       'OLD_pctch_all_li_count_90_00', 'OLD_ch_per_all_li_90_00',\n",
       "       'OLD_pctch_per_all_li_90_00', 'OLD_ch_real_hinc_90_00',\n",
       "       'OLD_pctch_real_hinc_90_00', 'region', 'OLD_rm_mrent_90',\n",
       "       'OLDaboverm_mrent_90', 'OLD_rm_per_all_li_90',\n",
       "       'OLD_aboverm_per_all_li_90', 'OLD_rm_per_nonwhite_90',\n",
       "       'OLD_aboverm_per_nonwhite_90', 'OLD_rm_per_rent_90',\n",
       "       'OLD_aboverm_per_rent_90', 'OLD_rm_per_col_90',\n",
       "       'OLD_aboverm_per_col_90', 'OLD_rm_per_col_17',\n",
       "       'OLD_aboverm_per_col_17', 'OLD_rm_ch_per_col_90_00',\n",
       "       'OLD_aboverm_ch_per_col_90_00', 'OLD_rm_ch_per_col_00_17',\n",
       "       'OLD_aboverm_ch_per_col_00_17', 'OLD_rm_per_rent_17',\n",
       "       'OLD_aboverm_per_rent_17', 'OLD_rm_per_nonwhite_17',\n",
       "       'OLD_aboverm_per_nonwhite_17', 'OLD_rm_per_all_li_17',\n",
       "       'OLD_aboverm_per_all_li_17', 'OLD_rm_pctch_real_mhval_90_00',\n",
       "       'OLD_aboverm_pctchreal_mhval90_00',\n",
       "       'OLD_rm_pctch_real_mhval_00_17',\n",
       "       'OLD_aboverm_pctchreal_mhval00_17',\n",
       "       'OLD_rm_pctch_real_mrent_90_00', 'OLD_abovermpctch_realmrent90_00',\n",
       "       'OLD_rm_pctch_real_mrent_00_17', 'OLD_abovermpctch_realmrent00_17',\n",
       "       'rm_per_limove_09', 'aboverm_per_limove_09',\n",
       "       'OLD_rm_per_limove_17', 'OLD_aboverm_per_limove_17',\n",
       "       'OLD_rm_ppt_ch_li_movein_09_17', 'OLD_aboverm_pptch_limovein09_17',\n",
       "       'OLD_rm_per_units_pre50_17', 'OLD_aboverm_per_units_pre50_17',\n",
       "       'OLD_rm_pctch_real_hinc_90_00', 'OLD_abovermpctch_realhinc90_00',\n",
       "       'OLD_rm_pctch_real_hinc_00_17', 'OLD_abovermpctch_realhinc00_17',\n",
       "       'OLD_rm_mrent_00', 'OLD_aboverm_mrent_00', 'OLD_rm_per_all_li_00',\n",
       "       'OLD_aboverm_perall_li00', 'OLD_rm_per_rent_00',\n",
       "       'OLD_aboverm_per_rent_00', 'OLD_rm_per_nonwhite_00',\n",
       "       'OLD_aboverm_per_nonwhite_00', 'OLD_rm_per_col_00',\n",
       "       'OLD_aboverm_per_col_00', 'OLD_rm_empd_17', 'OLD_aboverm_empd_17',\n",
       "       'OLD_rm_per_built_00_17', 'OLD_aboverm_per_built_00_17',\n",
       "       'OLD_rm_density_17', 'OLD_aboverm_density_17',\n",
       "       'OLD_rm_per_carcommute_17', 'OLD_aboverm_per_carcommute_17',\n",
       "       'OLD_rm_per_nhblk_17', 'OLD_aboverm_per_nhblk_17',\n",
       "       'OLD_rm_per_asian_17', 'OLD_aboverm_per_asian_17',\n",
       "       'OLD_rm_per_hisp_17', 'OLD_aboverm_per_hisp_17', 'OLD_pop17flag',\n",
       "       'OLD_pop00flag', 'OLD_pop_90flag', 'OLD_popgrowflag_17',\n",
       "       'OLD_hotmarket_17', 'OLD_hotmarket_00', 'OLD_gent1_17',\n",
       "       'OLD_gent2_17', 'OLD_gentrisk_17', 'OLD_gent1_00', 'OLD_gent2_00',\n",
       "       'OLD_gentrisk_00', 'aboverm_per_nwh_90_00',\n",
       "       'aboverm_per_renters_90_00', 'democh_17', 'democh_00',\n",
       "       'OLD_LI_vulnrisk_17_1', 'OLD_LI_vulnrisk_17_2',\n",
       "       'OLD_LI_vulnrisk_17', 'OLD_MHI_vulnrisk_17_1',\n",
       "       'OLD_MHI_vulnrisk_17_2', 'OLD_MHI_vulnrisk_17', 'OLD_lostli_17',\n",
       "       'OLD_lostli_00', 'OLD_limig17_lessthan09', 'OLD_typology',\n",
       "       'pop_17_cv', 'hinc_17_cv', 'hu_17_cv', 'per_col_17_cv',\n",
       "       'per_rent_17_cv', 'mrent_17_cv', 'mhval_17_cv', 'OLD_cvcat_pop_17',\n",
       "       'OLD_cvcat_hu_17', 'OLD_cvcat_mrent_17', 'OLD_cvcat_mhval_17',\n",
       "       'OLD_cvcat_hinc_17', 'OLD_cvcat_per_col_17',\n",
       "       'OLD_cvcat_per_rent_17', 'OLD_cv_typology', 'real_mrent_10',\n",
       "       'real_mhval_10', '_mergemhval_mrent10', 'occupiedunits',\n",
       "       'owneroccunits', 'ownerocc_2015orlater', 'ownerocc_2010to2014',\n",
       "       'ownerocc_2000to2009', 'ownerocc_1990to1999',\n",
       "       'ownerocc_1980to1989', 'ownerocc_1979orearlier', 'renteroccunits',\n",
       "       'renterocc_2015orlater', 'renterocc_2010to2014',\n",
       "       'renterocc_2000to2009', 'renterocc_1990to1999',\n",
       "       'renterocc_1980to1989', 'renterocc_1979orearlier',\n",
       "       'se_occupiedunits', 'se_owneroccunits', 'se_ownerocc_2015orlater',\n",
       "       'se_ownerocc_2010to2014', 'se_ownerocc_2000to2009',\n",
       "       'se_ownerocc_1990to1999', 'se_ownerocc_1980to1989',\n",
       "       'se_ownerocc_1979orearlier', 'se_renteroccunits',\n",
       "       'se_renterocc_2015orlater', 'se_renterocc_2010to2014',\n",
       "       'se_renterocc_2000to2009', 'se_renterocc_1990to1999',\n",
       "       'se_renterocc_1980to1989', 'se_renterocc_1979orearlier',\n",
       "       'pct_renter_recentmovein', 'pct_owner_recentmovein',\n",
       "       'pct_tot_recentmovein', 'pct_renter_recentmovein2010',\n",
       "       'pct_owner_recentmovein2010', 'pct_tot_recentmovein2010',\n",
       "       'rm_pct_recentmovein', 'aboverm_pct_recentmovein',\n",
       "       'rm_pct_recentmovein2010', 'aboverm_pct_recentmovein2010',\n",
       "       '_merge_2010movein', 'applicationrate', 'high_denialrate20',\n",
       "       'high_denialrate25', 'high_denialrate30', 'low_app_rate',\n",
       "       '_merge_HMDAdenial_appl_rate', 'presence_ph_LIHTC',\n",
       "       '_merge_subsidizedhousing', 'exclusive_tract', 'proximity',\n",
       "       '_merge_proximity', 'estimated_foreclosure_rate',\n",
       "       'rm_foreclosure_rate', 'aboverm_foreclosurerate',\n",
       "       '_merge_forclosure', 'LItract', 'homeownership_rm_LItracts',\n",
       "       'aboverm_homeownership_LItracts', '_merge_LI_homeowner',\n",
       "       'prison_pct', 'prison_flag', 'highprisonpop_flag', '_merge_prison',\n",
       "       'reg_med_inc60_17', 'reg_med_inc120_17', 'reg_med_inc80_17',\n",
       "       'low_pdmt_80120', 'high_pdmt_80120', 'mod_pdmt_80120',\n",
       "       'low_pdmt_55cut_80120_medhhinc', 'high_pdmt_55cut_80120_medhhinc',\n",
       "       'mod_pdmt_55cut_80120_medhhinc', 'mix_low_55cut_80120_medhhinc',\n",
       "       'mix_mod_55cut_80120_medhhinc', 'mix_high_55cut_80120_medhhinc',\n",
       "       'inc_cat_55cut_80120_medhhinc', '_merge_newinclevel',\n",
       "       'vacancy_rate', 'rm_vacancy', 'vacancy_rm_flag',\n",
       "       '_merge_vacancyrate17', 'per_change_HHs_2010_2017',\n",
       "       'per_change_units_2010_2017', 'flag_units_v_HHs',\n",
       "       'rm_units_2010_2017', 'flag_rm_units_change', '_merge_units_v_HHs',\n",
       "       'lmh_flag_new', 'aff_change_cat_full', 'ab_50pct_ch',\n",
       "       'ab_30pct_ch', 'ab_90percentile_ch', '_merge_aff', 'shape_length',\n",
       "       'shape_area', 'rail', '_merge_rail', 'per_own_90', 'per_own_00',\n",
       "       'per_own_17', 'reg_medinc_10', 'moveinincd_10', 'per_limove_10',\n",
       "       '_merge_2010mover_interp', 'hosp_fl', 'uni_fl',\n",
       "       'achor_institution', '_merge_anchor', 'stcty', 'pop_17_flag30',\n",
       "       'pop_17_flag15', 'rhu_17_cv', 'rhu_17_flag30', 'rhu_17_flag15',\n",
       "       'per_col_17_flag30', 'per_col_17_flag15', 'per_rent_17_flag30',\n",
       "       'per_rent_17_flag15', 'mrent_17_flag30', 'mrent_17_flag15',\n",
       "       'mhval_17_flag30', 'mhval_17_flag15', 'vli_flag',\n",
       "       'reg_med_inc80_00', 'reg_med_inc120_00', 'low_pdmt_80120_00',\n",
       "       'high_pdmt_80120_00', 'mod_pdmt_80120_00',\n",
       "       'low_pdmt_55cut_80120_medhhinc_00', 'high_pdmt_55cut_80120_00',\n",
       "       'mod_pdmt_55cut_80120_medhhinc_00',\n",
       "       'mix_low_55cut_80120_medhhinc_00',\n",
       "       'mix_mod_55cut_80120_medhhinc_00',\n",
       "       'mix_high_55cut_80120_medhhinc_00',\n",
       "       'inc_cat_55cut_80120_medhhinc_00', '_merge_inclevel_2000',\n",
       "       'lmh_flag_new_encoded', 'aff_change_cat_full_encoded'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memphis_data = pd.read_stata('clean_Memphis_merge_081319.dta')\n",
    "pd.Series(memphis_data.columns).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_00_cols = [\"TOD\", \"downtown\", \"per_asian_00\", \"mhval00\", \"empd02\", \"hh00\", \"hinc00\", \"hu_00\", \"per_black_00\", \"per_built_90_00\", \"per_car_commute_00\", \"per_col00\", \"per_hhwchild_00\", \"per_latino_00\", \"per_nonwhite00\", \"per_owners_00\", \"per_rent00\", \"per_units_pre50\", \"pop00\"]\n",
    "ny_90_cols = [\"TOD\", \"downtown\", \"per_asian_90\", \"mhval90\", \"empd02\", \"hh_90\", \"hinc90\", \"hu_90\", \"per_black_90\", \"per_built_80_90\", \"per_car_commute_90\", \"per_col90\", \"per_hhwchild_90\", \"per_latino_90\", \"per_nonwhite90\", \"per_owners_90\", \"per_rent90\", \"per_units_pre50\", \"pop90\"]\n",
    "\n",
    "#Keeping track of which variables I can find in Memphis dataset and not\n",
    "sf_00_cols = [\"tod\", \"emp_density00\", \"per_built90_00\", \"per_hhwchild_00\"]\n",
    "sf_90_cols = [\"tod\",  \"emp_density90\", \"per_built80_90\", \"per_hhwchild_90\"]\n",
    "\n",
    "mem_00_cols = ['per_nonwhite_00',\"downtown\",\"per_asian_00\",\"mhval00\",\"hh00\", \"hinc00\", \"per_hu_00\", \"per_nhblk_00\", \"per_carcommute_00\", \"per_col_00\",\"hhwchild_00\", \"per_hisp_00\", \"per_own_00\", \"per_rent_00\", \"pop00\",'per_units_pre50_00']\n",
    "mem_90_cols = ['per_nonwhite_90',\"downtown\",\"per_asian_90\",\"mhval90\", \"hh90\", \"hinc90\", \"per_hu_90\", \"per_nhblk_90\", \"per_carcommute_90\",\"per_col_90\", \"hhwchild90\",\"per_hisp_90\", \"per_own_90\", \"per_rent_90\", \"pop90\",'per_units_pre50_90' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_00_df = pd.read_excel(\"UDP_NY_2016_7.13.19_selectedNYNJcounties.xlsx\", sheet_name=\"2000 data\")\n",
    "ny_90_df = pd.read_excel(\"UDP_NY_2016_7.13.19_selectedNYNJcounties.xlsx\", sheet_name=\"1990 data\")\n",
    "\n",
    "sf_00_df = pd.read_excel(\"UDP_SF_2015_6.6.19.xlsx\", sheet_name=\"2000 data\")\n",
    "sf_90_df = pd.read_excel(\"UDP_SF_2015_6.6.19.xlsx\", sheet_name=\"1990 data\")\n",
    "\n",
    "mem_df = pd.read_stata('clean_Memphis_merge_081319.dta')\n",
    "#to_predict_mem = pd.read_stata('clean_Memphis_merge_081319.dta')\n",
    "\n",
    "\n",
    "to_predict_ny = pd.read_excel(\"UDP_NY_2016_7.13.19_selectedNYNJcounties.xlsx\", sheet_name=\"to predict\")\n",
    "\n",
    "# put these 2 collumn value lists in the same order\n",
    "predict_ny_90_cols = [\"gent90_00\", \"Disp_index_90_00_count\", \"Disp_index_90_00_pctch_count\", \"Disp_index_90_00_pctch_per\"]\n",
    "predict_ny_00_cols = [\"gent00_16\", \"Disp_index_00_16_count\", \"Disp_index_00_15_pctch_count\", \"Disp_index_00_15_pctch_per\"]\n",
    "\n",
    "predict_sf_90_cols = [\"gent90_00_v2\", \"Disp_index_90_00_count\", \"Disp_index_90_00_pctch_count\", \"Disp_index_90_00_pctch_per\"]\n",
    "predict_sf_00_cols = [\"gent00_15_v2\", \"Disp_index_00_15_count\", \"Disp_index_00_15_pctch_count\", \"Disp_index_00_15_pctch_per\"]\n",
    "\n",
    "#What these would look like in theory, currently have nothing to input (nothing to predict on)#\n",
    "predict_mem_90_cols = [\"gent90_00\", \"Disp_index_90_00_count\", \"Disp_index_90_00_pctch_count\", \"Disp_index_90_00_pctch_per\"]\n",
    "predict_mem_00_cols = [\"gent00_16\", \"Disp_index_00_16_count\", \"Disp_index_00_15_pctch_count\", \"Disp_index_00_15_pctch_per\"]\n",
    "\n",
    "to_predict_sf = pd.read_excel(\"UDP_SF_2015_6.6.19.xlsx\", sheet_name=\"to predict\")\n",
    "#predict_ny_00_cols = [\"gent00_15_v2\", \"Disp_index_90_00_count\", \"Disp_index_90_00_pctch_count\", \"Disp_index_90_00_pctch_per\", \"Disp_index_00_15_count\", \"Disp_index_00_15_pctch_count\", \"Disp_index_00_15_pctch_per\", \"Gent_index_90_00\", \"Gent_index_00_15\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what this WOULD look like for Memphis\n",
    "run_single_geography_model(mem_df, to_predict_mem, mem_90_cols, predict_mem_90_cols, \"1990s Memphis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memphis_data = pd.read_stata('clean_Memphis_merge_081319.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_type = pd.read_csv('typology_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memphis = pd.read_stata('memphis_typology_081319.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('UDP_NY_2016_7.13.19_selectedNYNJcounties.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
